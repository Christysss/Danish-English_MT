{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WqDvmzPtY3j_"
      },
      "source": [
        "# Transformer model - Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJtlswApZQyd",
        "outputId": "58e01fb5-e929-4cb2-925d-c8c81abea322"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cZtN-xCxUYI_"
      },
      "source": [
        "## 1. Data collection and processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgAzBFPa34OA",
        "outputId": "4b8319e2-ed0d-44b9-e724-70de327267d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('da_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "import spacy.cli\n",
        "\n",
        "spacy.cli.download(\"en_core_web_sm\")\n",
        "spacy.cli.download(\"da_core_news_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7Vo9aLW4JJI"
      },
      "outputs": [],
      "source": [
        "import da_core_news_sm\n",
        "import en_core_web_sm\n",
        "\n",
        "spacy_da = da_core_news_sm.load()\n",
        "spacy_en = en_core_web_sm.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IljSJYO34U1R"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from typing import Iterable, List\n",
        "\n",
        "SRC_LANGUAGE = 'da'\n",
        "TGT_LANGUAGE = 'en'\n",
        "# maps ordering of sentence pairs as they come in the dataset\n",
        "LANGUAGE_INDEX = {SRC_LANGUAGE: 1, TGT_LANGUAGE: 0}\n",
        "\n",
        "# Place-holders\n",
        "token_transform = {}\n",
        "vocab_transform = {}\n",
        "\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='da_core_news_sm')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3rQ-yoeoAU-",
        "outputId": "190723eb-861d-404f-dd12-d9d99107bb59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache/wiki.en.vec: 6.60GB [00:45, 144MB/s]                            \n",
            "  0%|          | 0/2519370 [00:00<?, ?it/s]WARNING:torchtext.vocab.vectors:Skipping token b'2519370' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|██████████| 2519370/2519370 [03:37<00:00, 11591.25it/s]\n",
            ".vector_cache/wiki.da.vec: 823MB [00:05, 145MB/s]                           \n",
            "  0%|          | 0/312956 [00:00<?, ?it/s]WARNING:torchtext.vocab.vectors:Skipping token b'312956' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|██████████| 312956/312956 [00:26<00:00, 11692.96it/s]\n"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import FastText\n",
        "\n",
        "fasttext_en = FastText(language=\"en\")\n",
        "fasttext_da = FastText(language=\"da\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qZ3Y4NQZace"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict, Counter\n",
        "from torchtext.vocab import vocab, Vectors\n",
        "import io\n",
        "\n",
        "path = './drive/My Drive/531_lab4/data/'\n",
        "train_da, train_en = 'train.da', 'train.en'\n",
        "valid_da, valid_en = 'dev.da', 'dev.en'\n",
        "test_da, test_en = 'test.da', 'test.en'\n",
        "\n",
        "def build_vocab(\n",
        "    filepath, \n",
        "    tokenizer, \n",
        "    min_freq=1, \n",
        "    specials=[\"unk\"], \n",
        "  ):\n",
        "  \"\"\"Generate vocabulary object for a given language.\"\"\"\n",
        "  counter = Counter()\n",
        "  with open(filepath, encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f.readlines()):\n",
        "      if i == 0:  # skip header\n",
        "        continue\n",
        "\n",
        "      out_tokens = line.strip(\"\\n\")\n",
        "      counter.update(tokenizer(out_tokens))\n",
        "    \n",
        "    # sort and wrap as OrderedDict\n",
        "    ordered = OrderedDict(sorted(counter.items(), key=lambda x: x[1], reverse=True))\n",
        "    \n",
        "    # build vocab object\n",
        "    v_obj = vocab(\n",
        "      ordered, \n",
        "      min_freq=min_freq, \n",
        "      specials=specials,\n",
        "    )\n",
        "    \n",
        "    return v_obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2RlAMh-374F"
      },
      "outputs": [],
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "SPECIAL_SYMBOLS = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "MIN_FREQ = 3\n",
        "\n",
        "file_mapping = {\n",
        "    SRC_LANGUAGE: [train_da, valid_da, test_da],\n",
        "    TGT_LANGUAGE: [train_en, valid_en, test_en]\n",
        "}\n",
        "\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "\n",
        "  vocab_transform[ln] = build_vocab(\n",
        "    path + file_mapping[ln][0],  # only training data\n",
        "    token_transform[ln],\n",
        "    min_freq=MIN_FREQ,\n",
        "    specials=SPECIAL_SYMBOLS,\n",
        "  )\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M6DqAKT7aWQ",
        "outputId": "819cabce-1fd2-48ef-b6b9-fdd42171fb1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique tokens in source (da) vocabulary: 12429\n",
            "Unique tokens in target (en) vocabulary: 11726\n"
          ]
        }
      ],
      "source": [
        "print(f\"Unique tokens in source (da) vocabulary: {len(vocab_transform[SRC_LANGUAGE])}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(vocab_transform[TGT_LANGUAGE])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWmKhP4Ilos7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = {\n",
        "  \"train\": 16,\n",
        "  \"valid\": 128,\n",
        "  \"test\": 128\n",
        "}\n",
        "\n",
        "def extract_transform(path, src_filename, tgt_filename):\n",
        "  \"\"\"Build the dataset containing paired sentences with source and target languages\"\"\"\n",
        "  src_raw_iter = iter(io.open(path + src_filename, encoding=\"utf-8\"))\n",
        "  tgt_raw_iter = iter(io.open(path + tgt_filename, encoding=\"utf-8\"))\n",
        "  data = []\n",
        "  for src_item, tgt_item in zip(src_raw_iter, tgt_raw_iter):\n",
        "    src_raw = src_item.strip(\"\\n\")\n",
        "    tgt_raw = tgt_item.strip(\"\\n\")\n",
        "    data.append((src_raw, tgt_raw))\n",
        "\n",
        "  return data\n",
        "\n",
        "train_data = extract_transform(path, train_da, train_en)\n",
        "val_data = extract_transform(path, valid_da, valid_en)\n",
        "test_data = extract_transform(path, test_da, test_en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F13Ml4H4th37",
        "outputId": "2a443a9b-a81c-4748-abd5-455bbc032d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 50000\n",
            "Number of validation examples: 2000\n",
            "Number of testing examples: 2000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training examples: {len(train_data)}\")\n",
        "print(f\"Number of validation examples: {len(val_data)}\")\n",
        "print(f\"Number of testing examples: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYKiI-iE7gZw",
        "outputId": "41b285e0-58a8-4415-d1fa-ab1a76b96112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Hvordan går det med din mor?', \"- how's your mom holding' up?\")\n",
            "('- En af os er nødt til at finde bilen!', '- One of us have to get to the car...')\n",
            "('under henvisning til Rådets forordning (EF) nr. 1405/2006 af 18. september 2006 om særlige foranstaltninger på landbrugsområdet til fordel for de mindre øer i Det Ægæiske Hav og om ændring af forordning (EF) nr. 1782/2003 [1], særlig artikel 14, og', 'Having regard to Council Regulation (EC) No 1405/2006 of 18 September 2006 laying down specific measures for agriculture in favour of the smaller Aegean islands and amending Regulation (EC) No 1782/2003 [1], and in particular Article 14 thereof,')\n"
          ]
        }
      ],
      "source": [
        "print(train_data[0])\n",
        "print(val_data[0])\n",
        "print(test_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj63C-cTcsd_",
        "outputId": "58395975-aa81-49f9-de4e-f3f7393897ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['-', 'En', 'af', 'os', 'er', 'nødt', 'til', 'at', 'finde', 'bilen', '!']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_transform[SRC_LANGUAGE]('- En af os er nødt til at finde bilen!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqcAIt-gdBMg",
        "outputId": "7a060860-8492-4f76-fbbb-bb6c07cb75bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['-En', 'af', 'os', 'er', 'nødt', 'til', 'at', 'finde', 'bilen', '!']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "token_transform[SRC_LANGUAGE]('-En af os er nødt til at finde bilen!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADp9WiLraT1m"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "  def func(txt_input):\n",
        "      for transform in transforms:\n",
        "          txt_input = transform(txt_input)\n",
        "      return txt_input\n",
        "  return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "  return torch.cat((torch.tensor([BOS_IDX]),\n",
        "                    torch.tensor(token_ids),\n",
        "                    torch.tensor([EOS_IDX])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZvIiV-wVQV_"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGMPjGI9qoKp"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    train_data, \n",
        "    batch_size=BATCH_SIZE[\"train\"], \n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    val_data, \n",
        "    batch_size=BATCH_SIZE[\"valid\"], \n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_data, \n",
        "    batch_size=BATCH_SIZE[\"test\"], \n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDXMjBInwCTr",
        "outputId": "27c1029e-694a-44c1-fe84-473d8cf59352"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor size of source language: torch.Size([24, 16])\n",
            "tensor size of target language: torch.Size([27, 16])\n",
            "the tensor of first example in target language: tensor([   2,   12, 1547,  112,    0,   15,    3,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "           1,    1,    1])\n"
          ]
        }
      ],
      "source": [
        "# batch example of training data\n",
        "for batch in train_dataloader:\n",
        "    src, trg = batch\n",
        "    print('tensor size of source language:', src.shape)\n",
        "    print('tensor size of target language:', trg.shape)\n",
        "    print('the tensor of first example in target language:', trg[:, 0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDr5Q7sb-Kr7",
        "outputId": "56cb304b-426e-4bae-9b79-195bfdf3ba88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('under henvisning til Rådets forordning (EF) nr. 1405/2006 af 18. september 2006 om særlige foranstaltninger på landbrugsområdet til fordel for de mindre øer i Det Ægæiske Hav og om ændring af forordning (EF) nr. 1782/2003 [1], særlig artikel 14, og',\n",
              " 'Having regard to Council Regulation (EC) No 1405/2006 of 18 September 2006 laying down specific measures for agriculture in favour of the smaller Aegean islands and amending Regulation (EC) No 1782/2003 [1], and in particular Article 14 thereof,')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Making sure that the first example correspond to the first line in the test file\n",
        "test_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMuHAQfR-qHX"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"./drive/My Drive/Colab Notebooks/ckpt_lab4/train_dataloader\", \"wb\") as f:\n",
        "     pickle.dump(train_dataloader, f)\n",
        "\n",
        "with open(\"./drive/My Drive/Colab Notebooks/ckpt_lab4/valid_dataloader\", \"wb\") as f:\n",
        "     pickle.dump(valid_dataloader, f)\n",
        "\n",
        "with open(\"./drive/My Drive/Colab Notebooks/ckpt_lab4/test_dataloader\", \"wb\") as f:\n",
        "     pickle.dump(test_dataloader, f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K1Hw-aqLs0L2"
      },
      "source": [
        "## 2. Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hwXRoepsNDx"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.nn import Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_ZejOQis2VY",
        "outputId": "ea0da22e-9846-4b22-aff7-76f0ec5b45ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HysRiTGss2Xo"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(\n",
        "          self,\n",
        "          emb_size: int,\n",
        "          dropout: float,\n",
        "          maxlen: int = 5000\n",
        "        ):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "        \n",
        "        self.register_buffer('pos_embedding', pos_embedding)  # needed by Transformer module\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    # def __init__(self, vocab_size: int, embedding: vocab.Vectors):\n",
        "    def __init__(self, emb_size, embedding: Vectors):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        #self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.embed = embedding\n",
        "        #self.emb_size = emb_size\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embed(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(\n",
        "          self,\n",
        "          num_encoder_layers: int,\n",
        "          num_decoder_layers: int,\n",
        "          emb_size: int,\n",
        "          nhead: int,\n",
        "          src_vocab_size: int,\n",
        "          tgt_vocab_size: int,\n",
        "          src_embedding: Vectors,\n",
        "          tgt_embedding: Vectors,\n",
        "          dim_feedforward: int = 512,\n",
        "          dropout: float = 0.1\n",
        "        ):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(\n",
        "            d_model=emb_size,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        # self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        # self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.src_tok_emb = TokenEmbedding(emb_size, src_embedding)\n",
        "        self.tgt_tok_emb = TokenEmbedding(emb_size, tgt_embedding)    \n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(\n",
        "          self,\n",
        "          src: Tensor,\n",
        "          trg: Tensor,\n",
        "          src_mask: Tensor,\n",
        "          tgt_mask: Tensor,\n",
        "          src_padding_mask: Tensor,\n",
        "          tgt_padding_mask: Tensor,\n",
        "          memory_key_padding_mask: Tensor\n",
        "        ):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        \n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "688Xe3l2s2Z3"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJFt8kXXtL8A",
        "outputId": "70a930ea-0549-40eb-f188-d50588af2c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding(12429, 300)\n",
            "Embedding(11726, 300)\n"
          ]
        }
      ],
      "source": [
        "# FastText vectors for source and target language\n",
        "src_tokens = [tok for tok, _ in vocab_transform[SRC_LANGUAGE].get_stoi().items()]\n",
        "tgt_tokens = [tok for tok, _ in vocab_transform[TGT_LANGUAGE].get_stoi().items()]\n",
        "\n",
        "src_ft_embeddings = fasttext_da.get_vecs_by_tokens(\n",
        "    src_tokens, \n",
        "    lower_case_backup=True\n",
        ")\n",
        "src_embeddings = torch.nn.Embedding.from_pretrained(src_ft_embeddings)\n",
        "\n",
        "tgt_ft_embeddings = fasttext_en.get_vecs_by_tokens(\n",
        "    tgt_tokens, \n",
        "    lower_case_backup=True\n",
        ")\n",
        "tgt_embeddings = torch.nn.Embedding.from_pretrained(tgt_ft_embeddings)\n",
        "\n",
        "print(src_embeddings)\n",
        "print(tgt_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckItfRrzs2cG"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(531)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 300  # FastText dim\n",
        "NHEAD = 6\n",
        "FFN_HID_DIM = 1024\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, src_embeddings,\n",
        "                                 tgt_embeddings, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_normal_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcpineo0OaxJ"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(model, iterator, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "\n",
        "    for src, tgt in tqdm(iterator):\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    for src, tgt in iterator:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkotVh7A7jTj",
        "outputId": "3c791c96-4f57-4a0e-fcb2-a3ec2905d38a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1,  Epoch time = 103.988s\n",
            "\t Train Loss: 5.280 | Train PPL: 196.296\n",
            "\t Val. Loss: 5.020 |  Val. PPL: 151.484\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2,  Epoch time = 103.549s\n",
            "\t Train Loss: 4.893 | Train PPL: 133.359\n",
            "\t Val. Loss: 4.794 |  Val. PPL: 120.823\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3,  Epoch time = 103.760s\n",
            "\t Train Loss: 4.702 | Train PPL: 110.173\n",
            "\t Val. Loss: 4.637 |  Val. PPL: 103.233\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4,  Epoch time = 103.643s\n",
            "\t Train Loss: 4.560 | Train PPL:  95.595\n",
            "\t Val. Loss: 4.507 |  Val. PPL:  90.638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5,  Epoch time = 103.496s\n",
            "\t Train Loss: 4.443 | Train PPL:  85.050\n",
            "\t Val. Loss: 4.435 |  Val. PPL:  84.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6,  Epoch time = 103.522s\n",
            "\t Train Loss: 4.349 | Train PPL:  77.374\n",
            "\t Val. Loss: 4.354 |  Val. PPL:  77.825\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7,  Epoch time = 103.079s\n",
            "\t Train Loss: 4.266 | Train PPL:  71.249\n",
            "\t Val. Loss: 4.282 |  Val. PPL:  72.416\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8,  Epoch time = 102.336s\n",
            "\t Train Loss: 4.192 | Train PPL:  66.179\n",
            "\t Val. Loss: 4.231 |  Val. PPL:  68.787\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9,  Epoch time = 102.185s\n",
            "\t Train Loss: 4.125 | Train PPL:  61.867\n",
            "\t Val. Loss: 4.173 |  Val. PPL:  64.885\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10,  Epoch time = 103.048s\n",
            "\t Train Loss: 4.064 | Train PPL:  58.196\n",
            "\t Val. Loss: 4.145 |  Val. PPL:  63.137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11,  Epoch time = 103.009s\n",
            "\t Train Loss: 4.005 | Train PPL:  54.894\n",
            "\t Val. Loss: 4.089 |  Val. PPL:  59.695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12,  Epoch time = 102.409s\n",
            "\t Train Loss: 3.954 | Train PPL:  52.121\n",
            "\t Val. Loss: 4.061 |  Val. PPL:  58.049\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13,  Epoch time = 102.119s\n",
            "\t Train Loss: 3.898 | Train PPL:  49.292\n",
            "\t Val. Loss: 4.019 |  Val. PPL:  55.671\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 14,  Epoch time = 102.873s\n",
            "\t Train Loss: 3.855 | Train PPL:  47.249\n",
            "\t Val. Loss: 3.985 |  Val. PPL:  53.789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15,  Epoch time = 103.406s\n",
            "\t Train Loss: 3.810 | Train PPL:  45.150\n",
            "\t Val. Loss: 3.951 |  Val. PPL:  52.004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16,  Epoch time = 103.701s\n",
            "\t Train Loss: 3.773 | Train PPL:  43.501\n",
            "\t Val. Loss: 3.919 |  Val. PPL:  50.347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17,  Epoch time = 103.444s\n",
            "\t Train Loss: 3.734 | Train PPL:  41.856\n",
            "\t Val. Loss: 3.908 |  Val. PPL:  49.793\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:44<00:00, 30.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 18,  Epoch time = 104.038s\n",
            "\t Train Loss: 3.691 | Train PPL:  40.088\n",
            "\t Val. Loss: 3.883 |  Val. PPL:  48.573\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:44<00:00, 30.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19,  Epoch time = 104.137s\n",
            "\t Train Loss: 3.662 | Train PPL:  38.933\n",
            "\t Val. Loss: 3.863 |  Val. PPL:  47.585\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 20,  Epoch time = 103.338s\n",
            "\t Train Loss: 3.626 | Train PPL:  37.575\n",
            "\t Val. Loss: 3.839 |  Val. PPL:  46.492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 21,  Epoch time = 102.837s\n",
            "\t Train Loss: 3.593 | Train PPL:  36.341\n",
            "\t Val. Loss: 3.837 |  Val. PPL:  46.401\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 22,  Epoch time = 102.800s\n",
            "\t Train Loss: 3.563 | Train PPL:  35.278\n",
            "\t Val. Loss: 3.810 |  Val. PPL:  45.128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 23,  Epoch time = 103.574s\n",
            "\t Train Loss: 3.534 | Train PPL:  34.246\n",
            "\t Val. Loss: 3.789 |  Val. PPL:  44.207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 24,  Epoch time = 102.737s\n",
            "\t Train Loss: 3.503 | Train PPL:  33.229\n",
            "\t Val. Loss: 3.803 |  Val. PPL:  44.849\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:41<00:00, 30.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 25,  Epoch time = 101.736s\n",
            "\t Train Loss: 3.475 | Train PPL:  32.300\n",
            "\t Val. Loss: 3.769 |  Val. PPL:  43.334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 26,  Epoch time = 102.727s\n",
            "\t Train Loss: 3.451 | Train PPL:  31.532\n",
            "\t Val. Loss: 3.755 |  Val. PPL:  42.713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:43<00:00, 30.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 27,  Epoch time = 103.051s\n",
            "\t Train Loss: 3.424 | Train PPL:  30.681\n",
            "\t Val. Loss: 3.746 |  Val. PPL:  42.366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 28,  Epoch time = 102.616s\n",
            "\t Train Loss: 3.398 | Train PPL:  29.894\n",
            "\t Val. Loss: 3.745 |  Val. PPL:  42.298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 29,  Epoch time = 102.328s\n",
            "\t Train Loss: 3.374 | Train PPL:  29.206\n",
            "\t Val. Loss: 3.742 |  Val. PPL:  42.191\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:41<00:00, 30.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 30,  Epoch time = 101.742s\n",
            "\t Train Loss: 3.354 | Train PPL:  28.608\n",
            "\t Val. Loss: 3.728 |  Val. PPL:  41.586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:41<00:00, 30.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 31,  Epoch time = 101.594s\n",
            "\t Train Loss: 3.336 | Train PPL:  28.116\n",
            "\t Val. Loss: 3.725 |  Val. PPL:  41.475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:41<00:00, 30.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 32,  Epoch time = 101.608s\n",
            "\t Train Loss: 3.314 | Train PPL:  27.482\n",
            "\t Val. Loss: 3.735 |  Val. PPL:  41.871\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 33,  Epoch time = 102.711s\n",
            "\t Train Loss: 3.301 | Train PPL:  27.131\n",
            "\t Val. Loss: 3.724 |  Val. PPL:  41.435\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 34,  Epoch time = 102.081s\n",
            "\t Train Loss: 3.287 | Train PPL:  26.765\n",
            "\t Val. Loss: 3.715 |  Val. PPL:  41.065\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:41<00:00, 30.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 35,  Epoch time = 101.844s\n",
            "\t Train Loss: 3.269 | Train PPL:  26.295\n",
            "\t Val. Loss: 3.729 |  Val. PPL:  41.653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:41<00:00, 30.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 36,  Epoch time = 101.702s\n",
            "\t Train Loss: 3.258 | Train PPL:  26.000\n",
            "\t Val. Loss: 3.723 |  Val. PPL:  41.370\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 37,  Epoch time = 102.946s\n",
            "\t Train Loss: 3.248 | Train PPL:  25.749\n",
            "\t Val. Loss: 3.727 |  Val. PPL:  41.558\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:42<00:00, 30.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 38,  Epoch time = 102.382s\n",
            "\t Train Loss: 3.243 | Train PPL:  25.614\n",
            "\t Val. Loss: 3.719 |  Val. PPL:  41.228\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:41<00:00, 30.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 39,  Epoch time = 101.869s\n",
            "\t Train Loss: 3.238 | Train PPL:  25.479\n",
            "\t Val. Loss: 3.715 |  Val. PPL:  41.040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [01:41<00:00, 30.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 40,  Epoch time = 101.770s\n",
            "\t Train Loss: 3.227 | Train PPL:  25.198\n",
            "\t Val. Loss: 3.745 |  Val. PPL:  42.295\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "NUM_EPOCHS = 40\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, train_dataloader, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer, valid_dataloader)\n",
        "\n",
        "    state_dict_model = transformer.state_dict()\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': state_dict_model,\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }\n",
        "\n",
        "    torch.save(state, \"./drive/MyDrive/Colab Notebooks/ckpt_lab4/transformer_\" + str(epoch) + \".pt\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print((f\"Epoch: {epoch},  \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    print(f'\\t Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wHnZYQm2kGF"
      },
      "outputs": [],
      "source": [
        "transformer.load_state_dict(torch.load('./drive/MyDrive/Colab Notebooks/ckpt_lab4/transformer_30.pt')['state_dict'])\n",
        "transformer = transformer.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgLROn7o9fiU",
        "outputId": "03151898-7e27-4f06-bd92-87b22309f982"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [02:06<00:00, 24.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1,  Epoch time = 126.817s\n",
            "\t Train Loss: 3.500 | Train PPL:  33.129\n",
            "\t Val. Loss: 3.805 |  Val. PPL:  44.934\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [02:03<00:00, 25.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2,  Epoch time = 123.958s\n",
            "\t Train Loss: 3.467 | Train PPL:  32.052\n",
            "\t Val. Loss: 3.803 |  Val. PPL:  44.843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [02:03<00:00, 25.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3,  Epoch time = 123.470s\n",
            "\t Train Loss: 3.441 | Train PPL:  31.211\n",
            "\t Val. Loss: 3.790 |  Val. PPL:  44.274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [02:03<00:00, 25.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4,  Epoch time = 123.945s\n",
            "\t Train Loss: 3.421 | Train PPL:  30.598\n",
            "\t Val. Loss: 3.783 |  Val. PPL:  43.935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [02:03<00:00, 25.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5,  Epoch time = 123.400s\n",
            "\t Train Loss: 3.403 | Train PPL:  30.043\n",
            "\t Val. Loss: 3.790 |  Val. PPL:  44.250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [02:03<00:00, 25.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6,  Epoch time = 123.575s\n",
            "\t Train Loss: 3.387 | Train PPL:  29.574\n",
            "\t Val. Loss: 3.770 |  Val. PPL:  43.379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [02:03<00:00, 25.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7,  Epoch time = 123.778s\n",
            "\t Train Loss: 3.374 | Train PPL:  29.199\n",
            "\t Val. Loss: 3.759 |  Val. PPL:  42.902\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [02:02<00:00, 25.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8,  Epoch time = 122.980s\n",
            "\t Train Loss: 3.364 | Train PPL:  28.918\n",
            "\t Val. Loss: 3.780 |  Val. PPL:  43.816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [02:02<00:00, 25.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9,  Epoch time = 122.869s\n",
            "\t Train Loss: 3.356 | Train PPL:  28.677\n",
            "\t Val. Loss: 3.783 |  Val. PPL:  43.928\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3125/3125 [02:02<00:00, 25.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10,  Epoch time = 122.649s\n",
            "\t Train Loss: 3.346 | Train PPL:  28.381\n",
            "\t Val. Loss: 3.772 |  Val. PPL:  43.447\n"
          ]
        }
      ],
      "source": [
        "# Further training for another 10 epochs as it still has room for convergence\n",
        "import gc\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, train_dataloader, optimizer)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer, valid_dataloader)\n",
        "\n",
        "    state_dict_model = transformer.state_dict()\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': state_dict_model,\n",
        "        'optimizer': optimizer.state_dict()\n",
        "    }\n",
        "\n",
        "    torch.save(state, \"./drive/MyDrive/Colab Notebooks/ckpt_lab4/transformer_\" + str(epoch + 20) + \".pt\")\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print((f\"Epoch: {epoch},  \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "    print(f'\\t Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ApLd07pVR8Uc"
      },
      "source": [
        "## 3. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I7H2De7VwyR"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(531)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "EMB_SIZE = 300 \n",
        "NHEAD = 6\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 4\n",
        "NUM_DECODER_LAYERS = 4\n",
        "\n",
        "transformer_best = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                      NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, src_embeddings,\n",
        "                                      tgt_embeddings, FFN_HID_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlEnN0BRVoYm"
      },
      "outputs": [],
      "source": [
        "transformer_best.load_state_dict(torch.load('./drive/MyDrive/Colab Notebooks/ckpt_lab4/transformer_36.pt')['state_dict'])\n",
        "transformer_best = transformer_best.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnVpSmDsL-Yr"
      },
      "outputs": [],
      "source": [
        "def inference(model, dataset):\n",
        "    '''\n",
        "    Function for translation inference\n",
        "\n",
        "    Input: \n",
        "    model: translation model;\n",
        "    dataset: list of tuples with raw sentence pairs.\n",
        "\n",
        "    Output:\n",
        "    Corpus BLEU score.\n",
        "    '''\n",
        "    from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "    all_translations = []\n",
        "    all_references = []\n",
        "    predictions_list = []\n",
        "    for src, tgt in tqdm(dataset):\n",
        "      out_translation = translate(model, src)\n",
        "      all_translations.append(out_translation.split())\n",
        "      all_references.append([token_transform[TGT_LANGUAGE](tgt)])\n",
        "      predictions_list.append(out_translation)\n",
        "\n",
        "    assert len(all_references) == len(all_translations)\n",
        "\n",
        "    corpus_bleu_score = corpus_bleu(all_references, all_translations)  \n",
        "    return corpus_bleu_score, predictions_list\n",
        "\n",
        "\n",
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len - 1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  \n",
        "        src, \n",
        "        src_mask, \n",
        "        max_len=num_tokens + 5, \n",
        "        start_symbol=BOS_IDX\n",
        "    ).flatten()\n",
        "\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-ad1-A3-gru",
        "outputId": "74d91f37-b951-401f-8060-1c620e833bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source sentence: under henvisning til Rådets forordning (EF) nr. 1405/2006 af 18. september 2006 om særlige foranstaltninger på landbrugsområdet til fordel for de mindre øer i Det Ægæiske Hav og om ændring af forordning (EF) nr. 1782/2003 [1], særlig artikel 14, og\n",
            "Our system translation:  Having regard to Council Regulation ( EC ) No <unk> of 20 September 2006 on the market of the market in the European Economic Community and amending Council Regulation ( EC ) No EC [ 1 ] , and in particular Article 6 thereof , \n",
            "Reference translation: Having regard to Council Regulation (EC) No 1405/2006 of 18 September 2006 laying down specific measures for agriculture in favour of the smaller Aegean islands and amending Regulation (EC) No 1782/2003 [1], and in particular Article 14 thereof,\n"
          ]
        }
      ],
      "source": [
        "test_src, test_tgt = test_data[0]\n",
        "out_translation = translate(transformer_best, test_src)\n",
        "# out_numericalized = text_transform[TGT_LANGUAGE](out_translation)[1:-1]\n",
        "# ref_numericalized = text_transform[TGT_LANGUAGE](test_tgt)\n",
        "print(f\"Source sentence: {test_src}\")\n",
        "print(f\"Our system translation: {out_translation}\")\n",
        "print(f\"Reference translation: {test_tgt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFzMQKdmLxxT"
      },
      "outputs": [],
      "source": [
        "out_trans_tokenized = out_translation.split()\n",
        "out_ref_tokenized = token_transform[TGT_LANGUAGE](test_tgt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUuCzVVPNUOX",
        "outputId": "20f3d978-d498-4d3b-a68f-56f3401bd7e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cumulative 1-gram: 0.695483\n",
            "Cumulative 2-gram: 0.596287\n",
            "Cumulative 3-gram: 0.509136\n",
            "Cumulative 4-gram: 0.444876\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "reference = [[token_transform[TGT_LANGUAGE](test_tgt)]]\n",
        "candidate = [out_translation.split()]\n",
        "\n",
        "print('Cumulative 1-gram: %f' % corpus_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
        "print('Cumulative 2-gram: %f' % corpus_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
        "print('Cumulative 3-gram: %f' % corpus_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
        "print('Cumulative 4-gram: %f' % corpus_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHW8djElOVZr",
        "outputId": "60325dd3-23f3-4a00-c936-b3b5e46a3b17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [02:57<00:00, 11.29it/s]\n"
          ]
        }
      ],
      "source": [
        "corpus_bleu_score, predictions_list = inference(transformer_best, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y6nbuOILDc9",
        "outputId": "62c57c1a-aaec-4978-f03b-045bd5c59ba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.09820819618879685\n"
          ]
        }
      ],
      "source": [
        "print(corpus_bleu_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "sPYjGM7--YBY",
        "outputId": "a8c09001-9a3e-4932-f770-d439d63ba59a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nsample output:\\nsubmission file for the first question successfully saved at ./drive/My Drive/Colab Notebooks/lab4/FIRSTNAME_LASTNAME.txt\\n'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "write the translations to a file. this function will help you generate the submission file for the first question.\n",
        "'''\n",
        "def out_prediction(save_directory, prediction_list):\n",
        "    \"\"\"\n",
        "    out_prediction takes four input varibles: first_name, last_name, save_directory, prediction_list\n",
        "    <first_name>, string, your first name, e.g., TOM\n",
        "    <last_name>, string, your last name, e.g., SMITH\n",
        "    <save_directory>, string, directory to save the submission file, e.g., ./drive/My Drive/Colab Notebooks/ckpt_mt_lab4\n",
        "    <prediction_list>, list of strings which includes all your predictions (or translations) of TEST samples\n",
        "          e.g., ['This is the translation of my first sentence', 'This is the translation of my second sentence',...]\n",
        "                    \n",
        "    Generate a file is named with <yourfirstname>_<yourlastname>.txt in save directory\n",
        "    \"\"\"\n",
        "    absolute_file_path = \"{}/{}_{}.txt\".format(save_directory)\n",
        "    output_file = open(absolute_file_path,'w', encoding=\"utf-8\")\n",
        "    for item in prediction_list:\n",
        "        output_file.write(item+\"\\n\")\n",
        "    output_file.close()\n",
        "    print(\"submission file for the first question successfully saved at %s\"%absolute_file_path)\n",
        "\n",
        "'''\n",
        "sample output:\n",
        "submission file for the first question successfully saved at ./drive/My Drive/Colab Notebooks/lab4/FIRSTNAME_LASTNAME.txt\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHhaHtEb-jng",
        "outputId": "6963964c-cebe-454b-b46b-0a9995badf14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "submission file for the first question successfully saved at /content/drive/MyDrive/Colab Notebooks/531_lab4/JUAN_ROESEL_2.txt\n"
          ]
        }
      ],
      "source": [
        "outpath = \"/content/drive/MyDrive/Colab Notebooks/531_lab4\"\n",
        "out_prediction(outpath, predictions_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOW1ngzh_ojG"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia6YfaXe-5cu",
        "outputId": "fbbada44-405f-4281-d272-b21a035701e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU = 5.41, 25.9/8.3/3.1/1.3 (BP=1.000, ratio=1.111, hyp_len=27916, ref_len=25134)\n",
            "It is not advisable to publish scores from multi-bleu.perl.  The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.  Instead you should detokenize then use mteval-v14.pl, which has a standard tokenization.  Scores from multi-bleu.perl can still be used for internal purposes when you have a consistent tokenizer.\n"
          ]
        }
      ],
      "source": [
        "!perl '/content/drive/MyDrive/Colab Notebooks/531_lab4/multi-bleu.perl' '/content/drive/MyDrive/Colab Notebooks/531_lab4/data/test.en' < '/content/drive/MyDrive/Colab Notebooks/531_lab4/JUAN_ROESEL_2.txt'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jQljGBIy-h30"
      },
      "source": [
        "## Reference:\n",
        "\n",
        "* https://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks\n",
        "\n",
        "* https://pytorch.org/docs/master/nn.html#transformerencoderlayer\n",
        "\n",
        "* https://pytorch.org/docs/master/_modules/torch/nn/modules/transformer.html#Transformer\n",
        "\n",
        "* https://pytorch.org/tutorials/beginner/translation_transformer.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO5c-u1G-igY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
